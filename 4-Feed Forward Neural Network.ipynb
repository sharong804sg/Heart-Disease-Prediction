{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52f66f66",
   "metadata": {},
   "source": [
    "Environment: Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe62b723",
   "metadata": {},
   "source": [
    "# <font color = 'purple'> Feed Forward Neural Network\n",
    "Predicting presence of heart disease using feed forward neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "309c1287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import optuna\n",
    "from optuna.trial import TrialState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eef9518",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_seed = 101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a882ba",
   "metadata": {},
   "source": [
    "## <font color = 'blue'> Reserve test data\n",
    "Test data will not be used for model training or hyperparameter tuning, itstead it is reserved for final evaluation of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4f86467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_df(fp, label_colname, my_seed=None):\n",
    "    \"\"\"\n",
    "    Function to import raw data, carry out pre-processing, and split into training and test datasets.\n",
    "    Test data will be reserved for final evaluation of model performance (i.e. not for hyperparameter tuning)\n",
    "    \n",
    "    :param fp: filepath\n",
    "    :param label_colname: name of column containing labels\n",
    "    :param my_seed: integer to be used to fix random state for train_test_split\n",
    "    \n",
    "    :return: tuple of dataframes - training_df, test_df\n",
    "    \"\"\"\n",
    "\n",
    "    # import data\n",
    "    df = pd.read_csv(fp)\n",
    "\n",
    "    # one-hot encoding of categorical variables\n",
    "    df = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "    # Standard scaling of features\n",
    "    # Note that doing this gives a marked improvement in validation accuracy from around 75% to around 90%\n",
    "    scaler = StandardScaler()\n",
    "    df[df.drop(columns=label_colname).columns] = scaler.fit_transform(df[df.drop(columns=label_colname).columns])\n",
    "\n",
    "\n",
    "    # separate into training & test datasets. \n",
    "    # Stratification is used to ensure training and test sets have representative proportions of all classes\n",
    "    training_df, test_df = train_test_split(df, test_size=0.2, random_state=my_seed, stratify=df[label_colname])\n",
    "\n",
    "    return training_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bf468d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df, test_df = get_train_test_df(fp = \"heart_clean.csv\", label_colname='HeartDisease', my_seed = my_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ebd914",
   "metadata": {},
   "source": [
    "## <font color = 'blue'> Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c8edd7",
   "metadata": {},
   "source": [
    "**Run Optuna Study to Tune Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9a6bb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset): # inherits properties of pytorch Dataset class\n",
    "    def __init__(self, dataframe, label_colname):\n",
    "        \"\"\"\n",
    "            Class initialisation\n",
    "            :param dataframe: pandas dataframe including features and labels\n",
    "            :param label_colname: name of column containing labels\n",
    "            \"\"\"\n",
    "        self.labels = dataframe[label_colname].to_numpy()\n",
    "        self.features = dataframe.drop(columns=[label_colname]).to_numpy()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        :return: length of dataset\n",
    "        \"\"\"\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Fetches features and label(s) at requested index\n",
    "        :param idx: requested index\n",
    "        :return: tuple of numpy arrays - batch_features, batch_labels\n",
    "        \"\"\"\n",
    "        batch_features = self.features[idx,:]\n",
    "        batch_labels = self.labels[idx]\n",
    "\n",
    "        return batch_features, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4430f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_dataloader(training_df, my_batchsize, label_colname, my_seed = None):\n",
    "    \"\"\"\n",
    "    Function to split training data into training and validation subsets and format as dataloaders\n",
    "    Model performance on validation set will be used for hyperparameter tuning.\n",
    "\n",
    "    :param training_df: dataframe with full set of training data\n",
    "    :param my_batchsize: batch size for pytorch DataLoader\n",
    "    :param label_colname: name of column containing labels\n",
    "    :param my_seed: optional integer to fix train test split random state\n",
    "\n",
    "    :return: tuple of pytorch DataLoaders - train_dataloader, val_dataloader\n",
    "    \"\"\"\n",
    "\n",
    "    # separate into training & validation datasets\n",
    "    train_data, val_data = train_test_split(training_df, test_size = 0.2, random_state = my_seed, stratify=training_df[label_colname])\n",
    "\n",
    "    #format as pytorch dataloader\n",
    "    train, val = MyDataset(train_data, label_colname), MyDataset(val_data, label_colname)\n",
    "    train_dataloader = DataLoader(train, batch_size=my_batchsize, shuffle=True)\n",
    "    val_dataloader = DataLoader(val, batch_size=my_batchsize)\n",
    "\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "667afe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameters(trial):\n",
    "    \"\"\"\n",
    "    Set parameters for neural network, optimisation algorithm etc.\n",
    "    \n",
    "    :param trial: Optuna trial object\n",
    "    \n",
    "    :return: dictionary of parameters:\n",
    "            - n_layers: number of layers in neural network\n",
    "            - n_units_l{i}: number of units in layer i\n",
    "            - dropout_l{i}: dropout probability for layer i\n",
    "            - lr: learning rate\n",
    "            - batch_size: batch size\n",
    "            - n_epochs: number of epochs (i.e. number of passes through training data to optimise weights)\n",
    "            - optimiser: optimisation algorithm to be used\n",
    "    \"\"\"\n",
    "    trial.suggest_int(\"n_layers\", 1, 3)\n",
    "\n",
    "    for i in range(trial.params['n_layers']):\n",
    "        trial.suggest_int(f'n_units_l{i}', 2, 20)\n",
    "        trial.suggest_float(f\"dropout_l{i}\", 0.1, 1)\n",
    "\n",
    "    trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    \n",
    "    # TODO: try optimising these as well\n",
    "    trial.suggest_int(\"batch_size\", 10, 10)\n",
    "    trial.suggest_int(\"n_epochs\", 30, 30)\n",
    "    trial.suggest_categorical(\"optimizer\",[\"SGD\"])\n",
    "\n",
    "    return trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20034a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = training_df.shape[1]-1  # number of features in feature matrix. \n",
    "n_classes = len(training_df['HeartDisease'].unique())  # number of unique classes. \n",
    "\n",
    "def define_model(my_params):\n",
    "    \"\"\"Defines feed-forward neural network based on set parameters\n",
    "    \n",
    "    :param my_params: dictionary of parameters (see set_parameters() for full list)\n",
    "    :return: nn model\n",
    "    \"\"\"\n",
    "\n",
    "    layers = []\n",
    "\n",
    "    in_features = n_features  # number of input features for 1st layer = no. of features in feature matrix\n",
    "    \n",
    "    for i in range(my_params['n_layers']):\n",
    "        # n_inputs = n_outputs of previous layer, n_outputs=no. of units in that lyr\n",
    "        out_features = my_params[f'n_units_l{i}']\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "\n",
    "        layers.append(nn.ReLU())  # activation function\n",
    "\n",
    "        #drop-out regularisation. (note: drop-out works by zeroing some elements of the tensor. tensor shape is unchanged)\n",
    "        p = my_params[f\"dropout_l{i}\"]\n",
    "        layers.append(nn.Dropout(p))\n",
    "\n",
    "        in_features = out_features  # no. of inputs for next layer = no. of outputs of this layer\n",
    "\n",
    "    layers.append(nn.Linear(in_features, n_classes))  # output layer. No. of outputs = no. of unique classes in dataset\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac4496c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_correct(predictions, y):\n",
    "    \"\"\"\n",
    "    Counts number of correct predictions in a batch\n",
    "    \n",
    "    :param predictions: 1D tensor with predictions\n",
    "    :param y: 1D tensor with true classes\n",
    "    \n",
    "    :return: number of correct predictions (pred==y)\n",
    "    \"\"\"\n",
    "    predictions = predictions.numpy()\n",
    "    y = y.numpy()\n",
    "\n",
    "    n_correct = (predictions == y).sum()\n",
    "\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e1a3e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective for Optuna to optimise\n",
    "    :param trial: Optuna trial object\n",
    "    :return: accuracy - fraction of correctly labelled validation points. This is what Optuna seeks to maximise\n",
    "    \"\"\"\n",
    "\n",
    "    #set parameters\n",
    "    my_params = set_parameters(trial)\n",
    "\n",
    "    # Instantiate model\n",
    "    model = define_model(my_params)\n",
    "\n",
    "    # Instantiate optimizer\n",
    "    optimizer_name = my_params['optimizer']\n",
    "    lr = my_params['lr']\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "\n",
    "    # get data\n",
    "    train_dataloader, val_dataloader = get_train_val_dataloader(training_df,\n",
    "                                                                my_batchsize=my_params['batch_size'],\n",
    "                                                                label_colname='HeartDisease')\n",
    "    # train model\n",
    "    for epoch in range(my_params['n_epochs']):\n",
    "\n",
    "        #train\n",
    "        model.train()\n",
    "        for batch, (X, y) in enumerate(train_dataloader):\n",
    "            # X and y are tensors. X.size() = (batch_size,n_features), y.size()=(batch_size,)\n",
    "            # set datatype for compatibility with nn.\n",
    "            X = X.float()\n",
    "            y = y.long()\n",
    "\n",
    "            # calculate model output and resulting loss\n",
    "            model_output = model(X)  # tensor. size=(batch_size x n_classes)\n",
    "            loss_fn = nn.CrossEntropyLoss() # instantiate loss function\n",
    "            loss = loss_fn(model_output, y)\n",
    "\n",
    "            # Backpropagation to update model weights\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # validate. We do this at each epoch to facilitate pruning:\n",
    "        # i.e. early termination of trials which are clearly not going to be optimum\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch, (X, y) in enumerate(val_dataloader):\n",
    "                X = X.float()\n",
    "                y = y.long()\n",
    "\n",
    "                # calculate model output and total number of correct predictions for this batch\n",
    "                model_output = model(X)\n",
    "                pred = torch.argmax(model_output, dim=1)  # prediction = class with highest output value\n",
    "                correct += count_correct(pred, y)\n",
    "\n",
    "        accuracy = correct / len(val_dataloader.dataset)\n",
    "\n",
    "        # report accuracy to allow Optuna to decide whether to prune this trial\n",
    "        trial.report(accuracy, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return accuracy # return final validation accuracy after all epochs (unless pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db497d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-17 13:08:45,600]\u001b[0m A new study created in memory with name: no-name-cc9f1f42-27d9-44f2-991c-246c42dd5f70\u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:08:47,186]\u001b[0m Trial 0 finished with value: 0.8367346938775511 and parameters: {'n_layers': 3, 'n_units_l0': 8, 'dropout_l0': 0.6825611872363542, 'n_units_l1': 11, 'dropout_l1': 0.3339420064687123, 'n_units_l2': 16, 'dropout_l2': 0.5552103657432551, 'lr': 0.011943411637326744, 'batch_size': 10, 'n_epochs': 30, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.8367346938775511.\u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:08:48,893]\u001b[0m Trial 1 finished with value: 0.5510204081632653 and parameters: {'n_layers': 2, 'n_units_l0': 8, 'dropout_l0': 0.6027422175579493, 'n_units_l1': 8, 'dropout_l1': 0.2798531293210851, 'lr': 5.3792830068769094e-05, 'batch_size': 10, 'n_epochs': 30, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.8367346938775511.\u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:08:51,152]\u001b[0m Trial 2 finished with value: 0.5510204081632653 and parameters: {'n_layers': 3, 'n_units_l0': 10, 'dropout_l0': 0.22778310880173344, 'n_units_l1': 10, 'dropout_l1': 0.6086820499910451, 'n_units_l2': 4, 'dropout_l2': 0.889769523928902, 'lr': 0.0014596066466703872, 'batch_size': 10, 'n_epochs': 30, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.8367346938775511.\u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:08:53,059]\u001b[0m Trial 3 finished with value: 0.8027210884353742 and parameters: {'n_layers': 2, 'n_units_l0': 10, 'dropout_l0': 0.6753769024697801, 'n_units_l1': 11, 'dropout_l1': 0.9186091848347148, 'lr': 0.02697944923895965, 'batch_size': 10, 'n_epochs': 30, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.8367346938775511.\u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:08:55,118]\u001b[0m Trial 4 finished with value: 0.7891156462585034 and parameters: {'n_layers': 2, 'n_units_l0': 3, 'dropout_l0': 0.5498319489507576, 'n_units_l1': 11, 'dropout_l1': 0.9470857046174539, 'lr': 0.07649360631359836, 'batch_size': 10, 'n_epochs': 30, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.8367346938775511.\u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:08:55,174]\u001b[0m Trial 5 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:08:55,247]\u001b[0m Trial 6 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:08:55,319]\u001b[0m Trial 7 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:08:55,386]\u001b[0m Trial 8 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:08:57,319]\u001b[0m Trial 9 finished with value: 0.8095238095238095 and parameters: {'n_layers': 2, 'n_units_l0': 13, 'dropout_l0': 0.1100975285202179, 'n_units_l1': 18, 'dropout_l1': 0.9280516055962857, 'lr': 0.02231363575437191, 'batch_size': 10, 'n_epochs': 30, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.8367346938775511.\u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:08:58,251]\u001b[0m Trial 10 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:00,607]\u001b[0m Trial 11 finished with value: 0.8299319727891157 and parameters: {'n_layers': 3, 'n_units_l0': 14, 'dropout_l0': 0.10717663824484089, 'n_units_l1': 19, 'dropout_l1': 0.4135460298097047, 'n_units_l2': 16, 'dropout_l2': 0.23242271706043116, 'lr': 0.06621306198002698, 'batch_size': 10, 'n_epochs': 30, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.8367346938775511.\u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:02,901]\u001b[0m Trial 12 finished with value: 0.8571428571428571 and parameters: {'n_layers': 3, 'n_units_l0': 7, 'dropout_l0': 0.42954340830870846, 'n_units_l1': 20, 'dropout_l1': 0.42030743092763473, 'n_units_l2': 16, 'dropout_l2': 0.19016048304096295, 'lr': 0.09418459492544475, 'batch_size': 10, 'n_epochs': 30, 'optimizer': 'SGD'}. Best is trial 12 with value: 0.8571428571428571.\u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:03,031]\u001b[0m Trial 13 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:03,258]\u001b[0m Trial 14 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:04,853]\u001b[0m Trial 15 finished with value: 0.8095238095238095 and parameters: {'n_layers': 1, 'n_units_l0': 7, 'dropout_l0': 0.8353319481059844, 'lr': 0.030489197085455444, 'batch_size': 10, 'n_epochs': 30, 'optimizer': 'SGD'}. Best is trial 12 with value: 0.8571428571428571.\u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:07,143]\u001b[0m Trial 16 finished with value: 0.8571428571428571 and parameters: {'n_layers': 3, 'n_units_l0': 9, 'dropout_l0': 0.4755654878157891, 'n_units_l1': 20, 'dropout_l1': 0.7195009060871389, 'n_units_l2': 10, 'dropout_l2': 0.11656942448863497, 'lr': 0.09850470819194002, 'batch_size': 10, 'n_epochs': 30, 'optimizer': 'SGD'}. Best is trial 12 with value: 0.8571428571428571.\u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:07,370]\u001b[0m Trial 17 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:07,518]\u001b[0m Trial 18 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:07,632]\u001b[0m Trial 19 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:09,075]\u001b[0m Trial 20 finished with value: 0.8843537414965986 and parameters: {'n_layers': 1, 'n_units_l0': 8, 'dropout_l0': 0.36207620316425937, 'lr': 0.0035655898813121436, 'batch_size': 10, 'n_epochs': 30, 'optimizer': 'SGD'}. Best is trial 20 with value: 0.8843537414965986.\u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:09,164]\u001b[0m Trial 21 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:09,280]\u001b[0m Trial 22 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:10,797]\u001b[0m Trial 23 finished with value: 0.8639455782312925 and parameters: {'n_layers': 1, 'n_units_l0': 7, 'dropout_l0': 0.21236369512059086, 'lr': 0.014528050311331, 'batch_size': 10, 'n_epochs': 30, 'optimizer': 'SGD'}. Best is trial 20 with value: 0.8843537414965986.\u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:12,479]\u001b[0m Trial 24 finished with value: 0.891156462585034 and parameters: {'n_layers': 1, 'n_units_l0': 12, 'dropout_l0': 0.2105886516700176, 'lr': 0.011403420795338043, 'batch_size': 10, 'n_epochs': 30, 'optimizer': 'SGD'}. Best is trial 24 with value: 0.891156462585034.\u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:12,618]\u001b[0m Trial 25 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:14,127]\u001b[0m Trial 26 finished with value: 0.8367346938775511 and parameters: {'n_layers': 1, 'n_units_l0': 16, 'dropout_l0': 0.18851666300943096, 'lr': 0.015523297040859049, 'batch_size': 10, 'n_epochs': 30, 'optimizer': 'SGD'}. Best is trial 24 with value: 0.891156462585034.\u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:14,209]\u001b[0m Trial 27 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:14,307]\u001b[0m Trial 28 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:15,964]\u001b[0m Trial 29 finished with value: 0.9047619047619048 and parameters: {'n_layers': 1, 'n_units_l0': 8, 'dropout_l0': 0.28062255705218997, 'lr': 0.00922086440196931, 'batch_size': 10, 'n_epochs': 30, 'optimizer': 'SGD'}. Best is trial 29 with value: 0.9047619047619048.\u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:16,042]\u001b[0m Trial 30 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:16,268]\u001b[0m Trial 31 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:17,981]\u001b[0m Trial 32 finished with value: 0.891156462585034 and parameters: {'n_layers': 1, 'n_units_l0': 8, 'dropout_l0': 0.15659949187424754, 'lr': 0.015967075622747677, 'batch_size': 10, 'n_epochs': 30, 'optimizer': 'SGD'}. Best is trial 29 with value: 0.9047619047619048.\u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:18,043]\u001b[0m Trial 33 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:18,123]\u001b[0m Trial 34 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:18,191]\u001b[0m Trial 35 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:19,895]\u001b[0m Trial 36 finished with value: 0.891156462585034 and parameters: {'n_layers': 1, 'n_units_l0': 9, 'dropout_l0': 0.5889445638272943, 'lr': 0.0358303036877923, 'batch_size': 10, 'n_epochs': 30, 'optimizer': 'SGD'}. Best is trial 29 with value: 0.9047619047619048.\u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:21,849]\u001b[0m Trial 37 finished with value: 0.8639455782312925 and parameters: {'n_layers': 2, 'n_units_l0': 10, 'dropout_l0': 0.6442358430056637, 'n_units_l1': 2, 'dropout_l1': 0.13709144630660852, 'lr': 0.051416094414086955, 'batch_size': 10, 'n_epochs': 30, 'optimizer': 'SGD'}. Best is trial 29 with value: 0.9047619047619048.\u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:21,940]\u001b[0m Trial 38 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:22,094]\u001b[0m Trial 39 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:22,185]\u001b[0m Trial 40 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:22,250]\u001b[0m Trial 41 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:22,388]\u001b[0m Trial 42 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:22,539]\u001b[0m Trial 43 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:22,934]\u001b[0m Trial 44 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:23,007]\u001b[0m Trial 45 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:23,098]\u001b[0m Trial 46 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:23,166]\u001b[0m Trial 47 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:23,328]\u001b[0m Trial 48 pruned. \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-17 13:09:23,416]\u001b[0m Trial 49 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:23,490]\u001b[0m Trial 50 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:23,558]\u001b[0m Trial 51 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:23,896]\u001b[0m Trial 52 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:24,042]\u001b[0m Trial 53 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:24,178]\u001b[0m Trial 54 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:24,251]\u001b[0m Trial 55 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:25,769]\u001b[0m Trial 56 finished with value: 0.9047619047619048 and parameters: {'n_layers': 1, 'n_units_l0': 9, 'dropout_l0': 0.850745135319485, 'lr': 0.06393213110898077, 'batch_size': 10, 'n_epochs': 30, 'optimizer': 'SGD'}. Best is trial 29 with value: 0.9047619047619048.\u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:25,885]\u001b[0m Trial 57 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:26,082]\u001b[0m Trial 58 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:26,159]\u001b[0m Trial 59 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:26,210]\u001b[0m Trial 60 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:26,374]\u001b[0m Trial 61 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:27,986]\u001b[0m Trial 62 finished with value: 0.9047619047619048 and parameters: {'n_layers': 1, 'n_units_l0': 7, 'dropout_l0': 0.2937246233736196, 'lr': 0.018001564512110005, 'batch_size': 10, 'n_epochs': 30, 'optimizer': 'SGD'}. Best is trial 29 with value: 0.9047619047619048.\u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:28,098]\u001b[0m Trial 63 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:28,267]\u001b[0m Trial 64 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:28,371]\u001b[0m Trial 65 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:28,536]\u001b[0m Trial 66 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:28,598]\u001b[0m Trial 67 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:28,674]\u001b[0m Trial 68 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:28,796]\u001b[0m Trial 69 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:30,665]\u001b[0m Trial 70 finished with value: 0.8775510204081632 and parameters: {'n_layers': 1, 'n_units_l0': 4, 'dropout_l0': 0.4982765892783101, 'lr': 0.07323221374377542, 'batch_size': 10, 'n_epochs': 30, 'optimizer': 'SGD'}. Best is trial 29 with value: 0.9047619047619048.\u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:32,464]\u001b[0m Trial 71 finished with value: 0.8435374149659864 and parameters: {'n_layers': 1, 'n_units_l0': 2, 'dropout_l0': 0.4910591639442776, 'lr': 0.0822823163052349, 'batch_size': 10, 'n_epochs': 30, 'optimizer': 'SGD'}. Best is trial 29 with value: 0.9047619047619048.\u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:33,703]\u001b[0m Trial 72 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:33,781]\u001b[0m Trial 73 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:34,681]\u001b[0m Trial 74 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:34,864]\u001b[0m Trial 75 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:34,941]\u001b[0m Trial 76 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:35,012]\u001b[0m Trial 77 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:35,108]\u001b[0m Trial 78 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:35,247]\u001b[0m Trial 79 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:35,371]\u001b[0m Trial 80 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:35,594]\u001b[0m Trial 81 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:35,710]\u001b[0m Trial 82 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:35,826]\u001b[0m Trial 83 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:35,945]\u001b[0m Trial 84 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:36,123]\u001b[0m Trial 85 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:36,264]\u001b[0m Trial 86 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:36,418]\u001b[0m Trial 87 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:36,594]\u001b[0m Trial 88 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:38,759]\u001b[0m Trial 89 finished with value: 0.8775510204081632 and parameters: {'n_layers': 2, 'n_units_l0': 10, 'dropout_l0': 0.5976516595579506, 'n_units_l1': 3, 'dropout_l1': 0.19347949270596443, 'lr': 0.04580581794890973, 'batch_size': 10, 'n_epochs': 30, 'optimizer': 'SGD'}. Best is trial 29 with value: 0.9047619047619048.\u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:38,867]\u001b[0m Trial 90 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:38,970]\u001b[0m Trial 91 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:39,126]\u001b[0m Trial 92 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:39,223]\u001b[0m Trial 93 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:39,373]\u001b[0m Trial 94 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:39,528]\u001b[0m Trial 95 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:39,658]\u001b[0m Trial 96 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:39,738]\u001b[0m Trial 97 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:39,864]\u001b[0m Trial 98 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-11-17 13:09:39,992]\u001b[0m Trial 99 pruned. \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# instantiate optuna study\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler())\n",
    "# Optimise hyperparameters will try {n_trials} param combinations or till {timeout} seconds is hit\n",
    "study.optimize(objective, n_trials=100, timeout=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce51989a",
   "metadata": {},
   "source": [
    "**Display Study Results & Extract Best Trial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d8311f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study statistics: \n",
      "  Number of finished trials:  100\n",
      "  Number of pruned trials:  77\n",
      "  Number of complete trials:  23\n",
      "\n",
      "Best trial:\n",
      "  Validation Accuracy:  0.9047619047619048\n",
      "  Params: \n",
      "    n_layers: 1\n",
      "    n_units_l0: 8\n",
      "    dropout_l0: 0.28062255705218997\n",
      "    lr: 0.00922086440196931\n",
      "    batch_size: 10\n",
      "    n_epochs: 30\n",
      "    optimizer: SGD\n"
     ]
    }
   ],
   "source": [
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"\\nBest trial:\")\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(\"  Validation Accuracy: \", best_trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26caa650",
   "metadata": {},
   "source": [
    "**Train Final Model Using Hyperparameters from Best Trial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "715c8919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataloader(df, my_batchsize):\n",
    "    \"\"\"\n",
    "    Function to format dataframe as dataloader\n",
    "    :param df: dataframe\n",
    "    :param my_batchsize: batch size for dataloader\n",
    "    :return: dataloader\n",
    "    \"\"\"\n",
    "    data = MyDataset(df, 'HeartDisease')\n",
    "    my_dataloader = DataLoader(data, batch_size=my_batchsize)\n",
    "\n",
    "    return my_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b75f6bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_final_model(my_params):\n",
    "    \"\"\"\n",
    "    Train final model using tuned hyperparameters from best Optuna trial\n",
    "    :param my_params: dictionary of parameters from Optuna trial object that had best validation accuracy\n",
    "\n",
    "    :return: pytorch neural network model\n",
    "    \"\"\"\n",
    "\n",
    "    # Instantiate model\n",
    "    model = define_model(my_params)\n",
    "\n",
    "    # Instantiate optimizer\n",
    "    optimizer_name = my_params['optimizer']\n",
    "    lr = my_params['lr']\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "\n",
    "    # get data. Entire training dataset is used here, including validation set\n",
    "    train_dataloader = df_to_dataloader(training_df, my_params['batch_size']) \n",
    "    \n",
    "    # train model\n",
    "    for epoch in range(my_params['n_epochs']):\n",
    "        model.train()\n",
    "        for batch, (X, y) in enumerate(train_dataloader):\n",
    "            # X and y are tensors. X.size() = (batch_size,n_features), y.size()=(batch_size,)\n",
    "            # set datatype for compatibility with nn.\n",
    "            X = X.float()\n",
    "            y = y.long()\n",
    "\n",
    "            # calculate model output and resulting loss\n",
    "            model_output = model(X)  # tensor. size=(batch_size x n_classes)\n",
    "            loss_fn = nn.CrossEntropyLoss()  # instantiate loss function\n",
    "            loss = loss_fn(model_output, y)\n",
    "\n",
    "            # Backpropagation to update model weights\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58c92073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_evaluate(model, df):\n",
    "    \"\"\"\n",
    "    Function to run trained and tuned model on provided dataframe to obtain predictions and evaluate\n",
    "    accuracy\n",
    "\n",
    "    :param model: trained model\n",
    "    :param df: dataframe including features and target/label\n",
    "    \n",
    "    :return: accuracy\n",
    "    \"\"\"\n",
    "    my_dataloader = df_to_dataloader(df, my_batchsize=10)\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(my_dataloader):\n",
    "            X = X.float()\n",
    "            y = y.long()\n",
    "\n",
    "            # calculate model output and total number of correct predictions for this batch\n",
    "            model_output = model(X)\n",
    "            pred = torch.argmax(model_output, dim=1)  # prediction = class with highest output value\n",
    "            correct += count_correct(pred, y)\n",
    "\n",
    "    accuracy = correct / len(my_dataloader.dataset)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "559c4761",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = best_trial.params\n",
    "final_model = train_final_model(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbfcb2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Final Training Accuracy: 0.8608458390177354\n"
     ]
    }
   ],
   "source": [
    "# Compute final training accuracy\n",
    "train_acc = predict_and_evaluate(final_model, training_df)\n",
    "print(f\"  Final Training Accuracy: {train_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cac3e9",
   "metadata": {},
   "source": [
    "## Evaluate Accuracy on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0a8b13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Test Accuracy: 0.875\n"
     ]
    }
   ],
   "source": [
    "test_acc = predict_and_evaluate(final_model, test_df)\n",
    "print(f\"  Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b285ad8",
   "metadata": {},
   "source": [
    "Test accuracy is close to training accuracy, indicating that we have not over-fit the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
